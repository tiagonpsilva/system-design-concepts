# üìê CAP Theorem (Teorema CAP)

## Defini√ß√£o e Vis√£o Geral

O Teorema CAP, tamb√©m conhecido como Teorema de Brewer (proposto por Eric Brewer em 2000), √© um princ√≠pio fundamental na teoria de sistemas distribu√≠dos. O teorema afirma que √© imposs√≠vel para um sistema de dados distribu√≠do garantir simultaneamente as tr√™s seguintes propriedades:

- **Consistency (Consist√™ncia)**: Todos os n√≥s veem os mesmos dados ao mesmo tempo. Ap√≥s uma opera√ß√£o de escrita, qualquer leitura subsequente retornar√° o valor atualizado.
- **Availability (Disponibilidade)**: O sistema continua operacional mesmo com falhas de n√≥s individuais, garantindo que toda requisi√ß√£o receba uma resposta, sem garantia de que contenha os dados mais recentes.
- **Partition Tolerance (Toler√¢ncia a Parti√ß√£o)**: O sistema continua funcionando apesar de falhas de comunica√ß√£o na rede que possam isolar partes do sistema umas das outras.

O teorema estabelece que, em um sistema distribu√≠do sujeito a parti√ß√µes de rede (o que √© inevit√°vel em sistemas reais), os designers precisam escolher entre consist√™ncia e disponibilidade. Isso resulta em tr√™s tipos principais de sistemas:

- **CP (Consist√™ncia + Toler√¢ncia a Parti√ß√£o)**: Prioriza consist√™ncia e toler√¢ncia a parti√ß√£o sobre disponibilidade.
- **AP (Disponibilidade + Toler√¢ncia a Parti√ß√£o)**: Prioriza disponibilidade e toler√¢ncia a parti√ß√£o sobre consist√™ncia.
- **CA (Consist√™ncia + Disponibilidade)**: Prioriza consist√™ncia e disponibilidade sobre toler√¢ncia a parti√ß√£o (raramente vi√°vel em sistemas distribu√≠dos reais).

## Diagramas

### O Tri√¢ngulo CAP

```mermaid
graph TD
    CAP((CAP<br>Theorem)) --- C[Consistency<br>Consist√™ncia]
    CAP --- A[Availability<br>Disponibilidade]
    CAP --- P[Partition Tolerance<br>Toler√¢ncia a Parti√ß√£o]
    
    C --- CP[CP<br>Sistemas Consistentes<br>e Tolerantes a Parti√ß√£o]
    A --- AP[AP<br>Sistemas Dispon√≠veis<br>e Tolerantes a Parti√ß√£o]
    C --- CA[CA<br>Sistemas Consistentes<br>e Dispon√≠veis]
    P --- CP
    P --- AP
    A --- CA
    
    style CAP fill:#f96,stroke:#333,stroke-width:2px
    style CP fill:#bbf,stroke:#333,stroke-width:1px
    style AP fill:#bfb,stroke:#333,stroke-width:1px
    style CA fill:#fbb,stroke:#333,stroke-width:1px
```

### Cen√°rio de Parti√ß√£o de Rede

```mermaid
sequenceDiagram
    participant Cliente
    participant N√≥1 as N√≥ 1
    participant N√≥2 as N√≥ 2
    
    Note over N√≥1,N√≥2: Sistema Operando Normalmente
    Cliente->>N√≥1: Escrita X = 1
    N√≥1->>N√≥2: Replica X = 1
    N√≥2-->>N√≥1: Confirma√ß√£o
    
    Note over N√≥1,N√≥2: Parti√ß√£o de Rede Ocorre
    rect rgb(255, 220, 220)
    Note over N√≥1,N√≥2: Comunica√ß√£o Interrompida
    end
    
    Cliente->>N√≥1: Escrita X = 2
    
    Note over N√≥1,N√≥2: Decis√£o do Sistema
    
    alt Sistema CP (Prioriza Consist√™ncia)
        N√≥1-->>Cliente: Rejeita Escrita (indispon√≠vel)
    else Sistema AP (Prioriza Disponibilidade)
        N√≥1-->>Cliente: Aceita Escrita (X = 2 no N√≥ 1)
        Cliente->>N√≥2: Leitura X?
        N√≥2-->>Cliente: X = 1 (inconsistente)
    end
```

### Sistemas e suas Caracter√≠sticas CAP

```mermaid
graph TD
    Systems[Sistemas Distribu√≠dos] --> CP[Sistemas CP]
    Systems --> AP[Sistemas AP]
    Systems --> CA[Sistemas CA*]
    
    CP --> MongoDB[MongoDB<br>com Configura√ß√£o W:majority]
    CP --> HBase[HBase]
    CP --> ZooKeeper[ZooKeeper]
    CP --> Etcd[etcd]
    
    AP --> Cassandra[Cassandra]
    AP --> CouchDB[CouchDB]
    AP --> DynamoDB[Amazon DynamoDB]
    AP --> Riak[Riak]
    
    CA --> RDBMS[RDBMS Tradicional<br>*em √önico N√≥]
    
    style CP fill:#bbf,stroke:#333,stroke-width:1px
    style AP fill:#bfb,stroke:#333,stroke-width:1px
    style CA fill:#fbb,stroke:#333,stroke-width:1px
```

## Casos de Uso

### Sistemas CP (Consistency + Partition Tolerance)
- **Sistemas Financeiros**: Aplica√ß√µes banc√°rias que exigem consist√™ncia rigorosa
- **Sistemas de Controle de Invent√°rio**: Onde a precis√£o dos dados √© cr√≠tica
- **Servi√ßos de Coordena√ß√£o**: Como ZooKeeper e etcd
- **Bancos de Dados com Fortes Garantias Transacionais**: Sistemas onde a integridade dos dados √© priorit√°ria

### Sistemas AP (Availability + Partition Tolerance)
- **Redes Sociais**: Onde disponibilidade cont√≠nua √© mais importante que consist√™ncia imediata
- **An√°lise de Big Data**: Sistemas que priorizam disponibilidade para leituras e escritas
- **Sistemas de Cache Distribu√≠do**: Onde dados temporariamente inconsistentes s√£o aceit√°veis
- **Sistemas de Alta Escalabilidade**: Aplica√ß√µes globais que precisam de disponibilidade cont√≠nua

### Sistemas CA (Consistency + Availability)
- **Bancos de Dados Relacionais Tradicionais**: Em um √∫nico n√≥
- **Sistemas Monol√≠ticos**: Aplica√ß√µes n√£o distribu√≠das
- **Nota**: Sistemas CA verdadeiros s√£o praticamente imposs√≠veis em redes reais, pois n√£o podem tolerar parti√ß√µes

## Exemplos Pr√°ticos

### MongoDB (Sistema CP)

```js
// Configura√ß√£o para priorizar consist√™ncia (CP)
db.runCommand({
    insert: "collection",
    documents: [ { x: 1 } ],
    writeConcern: { w: "majority", wtimeout: 5000 }
})

// Esta opera√ß√£o s√≥ ter√° sucesso se a maioria dos n√≥s confirmar a escrita
// Se ocorrer uma parti√ß√£o de rede, opera√ß√µes podem falhar (redu√ß√£o de disponibilidade)
```

### Cassandra (Sistema AP)

```sql
-- Configura√ß√£o para priorizar disponibilidade (AP)
CREATE TABLE users (
  user_id uuid PRIMARY KEY,
  name text,
  email text
) WITH CONSISTENCY LEVEL = ONE;

-- Leituras e escritas continuar√£o funcionando mesmo durante parti√ß√µes
-- Configura√ß√£o de n√≠vel de consist√™ncia menor (ONE) prioriza disponibilidade
-- Mas pode resultar em dados inconsistentes durante parti√ß√µes
```

### PostgreSQL com Replica√ß√£o (Aproxima√ß√£o de CA em cluster)

```sql
-- Configura√ß√£o para priorizar disponibilidade durante falhas
-- Mas n√£o tolera realmente parti√ß√µes de rede completas (n√£o √© verdadeiramente CA)
PRIMARY> SET synchronous_commit TO on; -- Garante consist√™ncia nas r√©plicas s√≠ncronas

-- N√≠vel de isolamento para garantir consist√™ncia
PRIMARY> SET default_transaction_isolation TO 'serializable';
```

## Modelos de Consist√™ncia Alternativos

Como o teorema CAP limita o que podemos alcan√ßar durante parti√ß√µes, v√°rios modelos de consist√™ncia alternativos foram desenvolvidos para oferecer compromissos mais refinados:

### Consist√™ncia Eventual

```mermaid
sequenceDiagram
    participant Cliente1 as Cliente 1
    participant ReplicaA as R√©plica A
    participant ReplicaB as R√©plica B
    participant Cliente2 as Cliente 2
    
    Cliente1->>ReplicaA: Escrita X = 1
    ReplicaA-->>Cliente1: Confirma√ß√£o
    Cliente2->>ReplicaB: Leitura X
    ReplicaB-->>Cliente2: X = 0 (valor antigo)
    
    Note over ReplicaA,ReplicaB: Propaga√ß√£o Ass√≠ncrona
    ReplicaA->>ReplicaB: Replica√ß√£o X = 1
    
    Cliente2->>ReplicaB: Leitura X (mais tarde)
    ReplicaB-->>Cliente2: X = 1 (consistente eventualmente)
```

### Consist√™ncia Causal

Garante que opera√ß√µes causalmente relacionadas sejam vistas por todos os n√≥s na mesma ordem.

### Consist√™ncia de Sess√£o

Garante que, dentro da mesma sess√£o de um cliente, as leituras reflitam as escritas anteriores.

## Pr√≥s e Contras

### Sistemas CP (Consistency + Partition Tolerance)

#### Pr√≥s
- Sempre retorna dados consistentes
- Garantias transacionais mais fortes
- Adequado para aplica√ß√µes cr√≠ticas onde a precis√£o √© fundamental
- Simplifica a l√≥gica da aplica√ß√£o (n√£o precisa lidar com inconsist√™ncias)

#### Contras
- Pode ficar indispon√≠vel durante parti√ß√µes de rede
- Geralmente oferece maior lat√™ncia
- Menor escalabilidade horizontal
- Pode rejeitar opera√ß√µes durante falhas

### Sistemas AP (Availability + Partition Tolerance)

#### Pr√≥s
- Alta disponibilidade mesmo durante falhas
- Baixa lat√™ncia
- Excelente escalabilidade horizontal
- Continua aceitando opera√ß√µes durante parti√ß√µes

#### Contras
- Pode retornar dados desatualizados ou inconsistentes
- Complexidade adicional para resolver conflitos
- A aplica√ß√£o precisa lidar com potenciais inconsist√™ncias
- Dif√≠cil implementar transa√ß√µes ACID completas

### Sistemas CA (Consistency + Availability)

#### Pr√≥s
- Modelo de dados e programa√ß√£o mais simples
- Suporte completo a transa√ß√µes ACID
- Familiar para a maioria dos desenvolvedores

#### Contras
- N√£o funciona bem em ambientes distribu√≠dos reais
- Limita√ß√µes fundamentais de escalabilidade
- Vulner√°vel a parti√ß√µes de rede (inevit√°veis em sistemas distribu√≠dos)

## Melhores Pr√°ticas

1. **Entenda os requisitos do seu dom√≠nio**: Algumas √°reas exigem consist√™ncia forte (financeira), outras podem tolerar inconsist√™ncias tempor√°rias (m√≠dias sociais).

2. **Considere a granularidade das escolhas CAP**: Diferentes partes do sistema podem fazer escolhas CAP diferentes.

3. **Implemente modelos de consist√™ncia refinados**: Use consist√™ncia eventual, causal ou de sess√£o quando apropriado.

4. **Projete para recupera√ß√£o de parti√ß√µes**: Tenha planos para reconcilia√ß√£o ap√≥s a resolu√ß√£o de parti√ß√µes.

5. **Use compensa√ß√£o em vez de transa√ß√µes distribu√≠das**: Em sistemas AP, implemente mecanismos de compensa√ß√£o para corrigir inconsist√™ncias.

6. **Monitore e alerte sobre parti√ß√µes**: Detecte parti√ß√µes de rede rapidamente para mitigar seus efeitos.

7. **Implemente idempot√™ncia**: Garanta que opera√ß√µes possam ser repetidas com seguran√ßa durante recupera√ß√£o.

8. **Considere abordagens de CQRS (Command Query Responsibility Segregation)**: Separe modelos de leitura e escrita para otimizar cada um para seus requisitos CAP.

## Al√©m do CAP: PACELC

O teorema PACELC estende o CAP, reconhecendo que mesmo quando n√£o h√° parti√ß√µes (P), ainda h√° uma escolha entre lat√™ncia (L) e consist√™ncia (C):

- **PA/EL**: Se houver uma parti√ß√£o (P), escolha disponibilidade (A); sen√£o (E), escolha baixa lat√™ncia (L)
- **PA/EC**: Se houver uma parti√ß√£o, escolha disponibilidade; sen√£o, escolha consist√™ncia
- **PC/EL**: Se houver uma parti√ß√£o, escolha consist√™ncia; sen√£o, escolha baixa lat√™ncia
- **PC/EC**: Se houver uma parti√ß√£o, escolha consist√™ncia; sen√£o, tamb√©m escolha consist√™ncia

## Aplica√ß√µes e Tecnologias

| Sistema | Modelo CAP | PACELC | Casos de Uso |
|---------|------------|--------|--------------|
| MongoDB | CP | PC/EC | Dados estruturados com necessidade de consultas complexas |
| Cassandra | AP | PA/EL | Dados altamente distribu√≠dos com prioridade para escrita |
| DynamoDB | AP | PA/EL | Aplica√ß√µes serverless com alta escala |
| PostgreSQL | CA* | PC/EC | Sistemas transacionais tradicionais |
| Redis Cluster | CP | PC/EC | Cache distribu√≠do com consist√™ncia |
| Memcached | AP | PA/EL | Cache distribu√≠do com foco em disponibilidade |
| HBase | CP | PC/EC | An√°lise de big data com consist√™ncia |
| Riak | AP | PA/EL | Armazenamento de conte√∫do distribu√≠do |
| CockroachDB | CP+ | PC/EC | Bancos SQL globalmente distribu√≠dos |
| Neo4j | CP | PC/EC | Bancos de dados de grafos |

\* Em um √∫nico n√≥  
\+ Com mecanismos adicionais para alta disponibilidade

## Refer√™ncias

- Gilbert, S., & Lynch, N. (2002). Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services. ACM SIGACT News, 33(2), 51-59.
- Brewer, E. (2012). CAP Twelve Years Later: How the "Rules" Have Changed. Computer, 45(2), 23-29.
- Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media.
- Abadi, D. (2012). Consistency Tradeoffs in Modern Distributed Database System Design. IEEE Computer Society.
- Bailis, P., & Ghodsi, A. (2013). Eventual Consistency Today: Limitations, Extensions, and Beyond. ACM Queue, 11(3).
- Vogels, W. (2009). Eventually Consistent. Communications of the ACM, 52(1), 40-44.
- Hamilton, J. (2007). On Designing and Deploying Internet-Scale Services. USENIX Large Installation System Administration Conference (LISA).
- Kingsbury, K. (2013). Jepsen: An Analysis of Distributed Systems Under Partition. https://aphyr.com/posts/281-jepsen-on-consistency
